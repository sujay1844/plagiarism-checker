{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# Load the data into a Pandas DataFrame\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "# Load the BERT tokenizer and encode the text data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode_plus(text, add_special_tokens=True, \n",
    "                                  max_length=64, padding='max_length', \n",
    "                                  return_attention_mask=True, return_tensors='tf')\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (dict(tokenize_text(text)), label) for text, label in zip(train_df['text'], train_df['label'])\n",
    ").batch(32)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (dict(tokenize_text(text)), label) for text, label in zip(val_df['text'], val_df['label'])\n",
    ").batch(32)\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Compile the model with appropriate loss and metrics\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ds, epochs=5, validation_data=val_ds)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
