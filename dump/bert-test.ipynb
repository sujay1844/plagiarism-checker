{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "# import torch\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_similarity(text1, text2):\n",
    "#     # Tokenize the texts\n",
    "#     encoded_dict1 = tokenizer.encode_plus(text1, add_special_tokens=True, \n",
    "#                                            max_length=64, padding='max_length', \n",
    "#                                            return_attention_mask=True, return_tensors='pt')\n",
    "#     encoded_dict2 = tokenizer.encode_plus(text2, add_special_tokens=True, \n",
    "#                                            max_length=64, padding='max_length', \n",
    "#                                            return_attention_mask=True, return_tensors='pt')\n",
    "#     # Get the model's prediction\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(encoded_dict1['input_ids'], attention_mask=encoded_dict1['attention_mask'], \n",
    "#                        token_type_ids=None)[0]\n",
    "#         pred1 = torch.softmax(logits, dim=1)[0][1].item()\n",
    "#         logits = model(encoded_dict2['input_ids'], attention_mask=encoded_dict2['attention_mask'], \n",
    "#                        token_type_ids=None)[0]\n",
    "#         pred2 = torch.softmax(logits, dim=1)[0][1].item()\n",
    "#     # Return the similarity score\n",
    "#     return max(pred1, pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_similarity(text1, text2):\n",
    "    # Tokenize the texts\n",
    "    encoded_dict1 = tokenizer.encode_plus(text1, add_special_tokens=True, \n",
    "                                           max_length=64, padding='max_length', \n",
    "                                           return_attention_mask=True, return_tensors='tf')\n",
    "    encoded_dict2 = tokenizer.encode_plus(text2, add_special_tokens=True, \n",
    "                                           max_length=64, padding='max_length', \n",
    "                                           return_attention_mask=True, return_tensors='tf')\n",
    "    # Get the model's prediction\n",
    "    logits1 = model(encoded_dict1['input_ids'], attention_mask=encoded_dict1['attention_mask'], \n",
    "                    token_type_ids=None)[0]\n",
    "    pred1 = tf.nn.softmax(logits1, axis=1).numpy()[0][1]\n",
    "    logits2 = model(encoded_dict2['input_ids'], attention_mask=encoded_dict2['attention_mask'], \n",
    "                    token_type_ids=None)[0]\n",
    "    pred2 = tf.nn.softmax(logits2, axis=1).numpy()[0][1]\n",
    "    # Return the similarity score\n",
    "    return max(pred1, pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33824262\n"
     ]
    }
   ],
   "source": [
    "raw = \"Two women who just had lunch hugging and saying goodbye.\tThere are two woman in this picture.\t1\"\n",
    "text1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "text2 = \"The lazy dog is jumped over by the quick brown fox.\"\n",
    "text1, text2, label = raw.split('\\t')\n",
    "similarity = predict_similarity(text1, text2)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(raw):\n",
    "\ttext1, text2, label = raw.split('\\t')\n",
    "\tsimilarity = predict_similarity(text1, text2)\n",
    "\treturn similarity, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.36488363, '0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check(\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\tA blond man wearing a brown shirt is reading a book on a bench in the park\t0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29347184, '1')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check(\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\tA blond man drinking water from a fountain.\t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newConda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
